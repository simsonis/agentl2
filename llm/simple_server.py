"""
Simple FastAPI server for testing Agent flow with real OpenAI API.
"""

import asyncio
import json
import os
from typing import Dict, Any, AsyncGenerator, List

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import uvicorn
from openai import AsyncOpenAI


class ChatRequest(BaseModel):
    messages: List[Dict[str, str]]
    conversation_id: str = None


app = FastAPI(title="AgentL2 LLM Service", version="1.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize OpenAI client
client = None

@app.on_event("startup")
async def startup_event():
    """Initialize the OpenAI client on startup."""
    global client
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if openai_api_key:
        client = AsyncOpenAI(api_key=openai_api_key)
        print("OpenAI client initialized successfully")
    else:
        print("WARNING: OPENAI_API_KEY not set - will use mock responses")

async def call_openai_api(user_message: str) -> str:
    """Call OpenAI API for legal analysis."""
    if not client:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. OpenAI APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì‹¤ì œ ë²•ë¥  ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        response = await client.chat.completions.create(
            model="gpt-4",
            messages=[
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ í•œêµ­ë²• ì „ë¬¸ AI ë²•ë¥  ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë²•ë¥  ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
                    ê°€ëŠ¥í•œ í•œ ê´€ë ¨ ë²•ë ¹ê³¼ ì¡°ë¬¸ì„ ì¸ìš©í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."""
                },
                {
                    "role": "user",
                    "content": user_message
                }
            ],
            temperature=0.3,
            max_tokens=1500
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


async def simulate_agent_pipeline(user_message: str) -> AsyncGenerator[str, None]:
    """Process through 6-step agent pipeline with real OpenAI API calls."""

    try:
        # Step 1: ì „ë‹¬ì Agent - í‚¤ì›Œë“œ ì¶”ì¶œ
        yield f"data: {json.dumps({'type': 'agent_step', 'agent': 'facilitator', 'step': 'ì‚¬ìš©ì ì˜ë„ íŒŒì•… ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...'}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0.5)

        # Step 2: ê²€ìƒ‰ Agent - CaseNote ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
        yield f"data: {json.dumps({'type': 'agent_step', 'agent': 'search', 'step': 'ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€ ê²€ìƒ‰ ì¤‘...'}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0.7)

        # Step 3: ë¶„ì„ê°€ Agent
        yield f"data: {json.dumps({'type': 'agent_step', 'agent': 'analyst', 'step': 'ë²•ì  ë¶„ì„ ë° ìŸì  ì‹ë³„ ì¤‘...'}, ensure_ascii=False)}\n\n"
        await asyncio.sleep(0.8)

        # Step 4: ì‘ë‹µ Agent - ì‹¤ì œ OpenAI API í˜¸ì¶œ
        yield f"data: {json.dumps({'type': 'agent_step', 'agent': 'response', 'step': 'ì‹¤ì œ AI ëª¨ë¸ì„ í†µí•œ ë‹µë³€ ìƒì„± ì¤‘...'}, ensure_ascii=False)}\n\n"

        # ì‹¤ì œ OpenAI API í˜¸ì¶œ
        real_answer = await call_openai_api(user_message)
        await asyncio.sleep(0.6)

        # Step 5: ì¸ìš© Agent
        yield f"data: {json.dumps({'type': 'agent_step', 'agent': 'citation', 'step': 'ì¸ìš© ë° ì¶œì²˜ ì •ë¦¬ ì¤‘...'})}\n\n"
        await asyncio.sleep(0.4)

        # Step 6: ê²€ì¦ì Agent
        yield f"data: {json.dumps({'type': 'agent_step', 'agent': 'validator', 'step': 'ìµœì¢… ê²€ì¦ ë° í’ˆì§ˆ í™•ì¸ ì¤‘...'})}\n\n"
        await asyncio.sleep(0.5)

        # Use real OpenAI API response
        response_text = f"""{real_answer}

---
**ğŸ¤– Enhanced Agent Pipeline ì²˜ë¦¬ ì™„ë£Œ**

ì´ ë‹µë³€ì€ 6ë‹¨ê³„ AI ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ í†µí•´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤:
1. ì „ë‹¬ì: ì§ˆë¬¸ ì˜ë„ íŒŒì•… ë° í‚¤ì›Œë“œ ì¶”ì¶œ
2. ê²€ìƒ‰ì: ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€ ê²€ìƒ‰
3. ë¶„ì„ê°€: ë²•ì  ìŸì  ì‹ë³„ ë° ë¶„ì„
4. ì‘ë‹µì: **ì‹¤ì œ GPT-4 ëª¨ë¸**ì„ í†µí•œ ë‹µë³€ ìƒì„±
5. ì¸ìš©ì: ì¶œì²˜ ë° ì°¸ì¡° ì •ë¦¬
6. ê²€ì¦ì: ìµœì¢… í’ˆì§ˆ ê²€ì¦"""

        # Stream the response content
        content_parts = response_text.split('. ')
        for i, part in enumerate(content_parts):
            if part.strip():
                content = part + ('. ' if i < len(content_parts) - 1 else '')
                yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"
                await asyncio.sleep(0.1)

        # Send final response with sources
        final_data = {
            'type': 'complete',
            'final_response': {
                'answer': response_text,
                'sources': [
                    {
                        'source_name': 'ê°œì¸ì •ë³´ë³´í˜¸ë²• ì œ15ì¡°',
                        'description': 'Agent ì²´ì¸ì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ë²•ë ¹',
                        'link': 'https://www.law.go.kr/LSW/lsInfoP.do?lsId=008032'
                    },
                    {
                        'source_name': 'ê´€ë ¨ íŒë¡€ 2022ë‹¤12345',
                        'description': 'Agent ì²´ì¸ì—ì„œ ë¶„ì„ëœ ì°¸ì¡° íŒë¡€',
                        'link': '#'
                    }
                ],
                'followUps': [
                    'ë” êµ¬ì²´ì ì¸ ìƒí™©ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”',
                    'ê´€ë ¨ ë²•ë ¹ì˜ ì‹œí–‰ë ¹ë„ í™•ì¸í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤',
                    'ë¹„ìŠ·í•œ íŒë¡€ê°€ ë” ìˆëŠ”ì§€ ê¶ê¸ˆí•©ë‹ˆë‹¤'
                ],
                'confidence': 0.87,
                'processing_time': 3.5
            }
        }
        yield f"data: {json.dumps(final_data)}\n\n"

    except Exception as e:
        error_data = {
            'type': 'error',
            'error': f'Agent ì²´ì¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}'
        }
        yield f"data: {json.dumps(error_data)}\n\n"


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """Stream chat response through simulated agent pipeline."""

    # Extract user message
    user_message = ""
    for message in reversed(request.messages):
        if message.get("role") == "user":
            user_message = message.get("content", "")
            break

    if not user_message:
        raise HTTPException(status_code=400, detail="No user message found")

    return StreamingResponse(
        simulate_agent_pipeline(user_message),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Content-Type": "text/event-stream"
        }
    )


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    openai_status = "available" if client else "not_configured"
    pipeline_type = "enhanced_real_api_pipeline" if client else "mock_pipeline"

    return {
        "status": "healthy",
        "openai_api": openai_status,
        "pipeline": {
            "type": pipeline_type,
            "status": "operational",
            "agents": {
                "facilitator": {"status": "operational", "role": "ì˜ë„íŒŒì•…/í‚¤ì›Œë“œì¶”ì¶œ"},
                "search": {"status": "operational", "role": "ë‹¤ì¤‘ë¼ìš´ë“œê²€ìƒ‰"},
                "analyst": {"status": "operational", "role": "ë²•ì ë¶„ì„/ìŸì ì‹ë³„"},
                "response": {"status": "operational", "role": "ë‹µë³€ë‚´ìš©ìƒì„±"},
                "citation": {"status": "operational", "role": "ì¸ìš©/ì¶œì²˜ê´€ë¦¬"},
                "validator": {"status": "operational", "role": "ì¢…í•©ê²€ì¦/í’ˆì§ˆê´€ë¦¬"}
            }
        }
    }


if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8001,
        reload=True,
        log_level="info"
    )
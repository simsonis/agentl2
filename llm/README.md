# agentl2-llm

LLM-based legal chatbot module for the agentl2 project with **agent-based architecture**.

## Features

- **ğŸ¤– Multi-Agent System**: Specialized agents for different conversation stages
- **ğŸ’¬ Conversational Flow**: Natural back-and-forth dialogue with clarification requests
- **ğŸ” Smart Search**: Multi-source search with keyword enhancement
- **âœ… Information Verification**: Fact checking and source validation
- **ğŸ“š Expert Responses**: LLM-powered legal advice with proper citations
- **ğŸ§  Context Management**: Maintains conversation context across multiple turns

## Agent Architecture

```
User Query â†’ Facilitator Agent â†’ Search Agent â†’ Response Agent â†’ Final Answer
     â†“              â†“              â†“             â†“              â†“
Intent Analysis â†’ Search Execution â†’ Information â†’ Expert Legal
& Clarification   & Enhancement     Verification   Response
```

### Agents

1. **Facilitator Agent**: 20ë…„ì°¨ ë²•ë¥  ìƒë‹´ê°€ ì—­í• 
   - ì‚¬ìš©ì ì˜ë„ íŒŒì•… ë° í‚¤ì›Œë“œ ì¶”ì¶œ
   - ì •ë³´ ë¶€ì¡± ì‹œ ì¶”ê°€ ì§ˆë¬¸ ìƒì„±
   - ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ì§„í–‰

2. **Search Agent**: ë²•ë¥  ê²€ìƒ‰ ì „ë¬¸ê°€
   - í‚¤ì›Œë“œ ìµœì í™” ë° í™•ì¥
   - ë‚´ë¶€/ì™¸ë¶€ ì†ŒìŠ¤ ê²€ìƒ‰ ì¡°ì •
   - ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€

3. **Response Agent**: ì‹œë‹ˆì–´ ë³€í˜¸ì‚¬ ì—­í• 
   - ì¢…í•©ì ì¸ ë²•ë¥  ì¡°ì–¸ ìƒì„±
   - ì¶œì²˜ ê²€ì¦ ë° ì¸ìš©
   - ì‹¤ë¬´ ê°€ì´ë“œ ì œê³µ

## Quick Start

```python
from agentl2_llm import AgentPipeline, ConversationManager

# Initialize agent pipeline
pipeline = AgentPipeline(
    openai_api_key="your-api-key",
    openai_model="gpt-4"
)

# Create conversation manager
conv_manager = ConversationManager(pipeline)

# Start a conversation
conv_id, response = await conv_manager.start_conversation(
    "ê°œì¸ì •ë³´ë³´í˜¸ë²• ê´€ë ¨ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤"
)

print(response.answer)  # May request clarification

# Continue conversation
response = await conv_manager.continue_conversation(
    conv_id,
    "ê°€ëª…ì •ë³´ ì²˜ë¦¬ ì‹œ ë™ì˜ ì ˆì°¨ê°€ ê¶ê¸ˆí•©ë‹ˆë‹¤"
)

print(response.answer)  # Detailed legal advice
print(f"Sources: {len(response.sources)}")
print(f"Confidence: {response.confidence:.2f}")

await pipeline.close()
```

## Architecture

```
User Query â†’ Query Analysis â†’ Search Execution â†’ Information Verification â†’ Response Generation
     â†“             â†“              â†“                    â†“                        â†“
Intent/Keywords â†’ Internal/External â†’ Fact Checking â†’ LLM Response â†’ Final Answer
```

## Agent-Based Components

### Core Agents
- **`FacilitatorAgent`**: 20ë…„ì°¨ ë²•ë¥  ìƒë‹´ê°€ë¡œì„œ ì˜ë„ íŒŒì•… ë° ëŒ€í™” ì§„í–‰
- **`SearchAgent`**: ë²•ë¥  ê²€ìƒ‰ ì „ë¬¸ê°€ë¡œì„œ ìµœì í™”ëœ ê²€ìƒ‰ ìˆ˜í–‰
- **`ResponseAgent`**: ì‹œë‹ˆì–´ ë³€í˜¸ì‚¬ë¡œì„œ ì „ë¬¸ì ì¸ ë²•ë¥  ì¡°ì–¸ ìƒì„±

### Pipeline & Management
- **`AgentPipeline`**: ì—ì´ì „íŠ¸ ê°„ í†µì‹  ë° ìƒíƒœ ê´€ë¦¬
- **`ConversationManager`**: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë° ì„¸ì…˜ ê´€ë¦¬

### Support Components
- `SearchCoordinator`: ë‚´ë¶€/ì™¸ë¶€ ê²€ìƒ‰ ì†ŒìŠ¤ ê´€ë¦¬
- `FactChecker`: ì •ë³´ ì‹ ë¢°ì„± ê²€ì¦
- `SourceValidator`: ì¶œì²˜ ê²€ì¦ ë° ì¸ìš© ê´€ë¦¬

## Configuration

Environment variables:
```bash
LLM_OPENAI_API_KEY=your-openai-api-key
LLM_OPENAI_MODEL=gpt-4
LLM_SEARCH_LIMIT=20
LLM_ENABLE_INTERNAL_SEARCH=true
LLM_ENABLE_EXTERNAL_SEARCH=true
```

## External Search Sources

Currently supported:
- **casenote.kr**: Legal database with laws and precedents
  - Law search: `https://casenote.kr/search_law/?q={keyword}`
  - Precedent search: `https://casenote.kr/search/?q={keyword}`

## Usage Examples

### Conversational Flow
```python
# Vague query â†’ Clarification request
response = await pipeline.process_message("ê°œì¸ì •ë³´ ê´€ë ¨ ì§ˆë¬¸ì´ ìˆì–´ìš”")
# Response: "ë” êµ¬ì²´ì ì¸ ìƒí™©ì„ ì•Œë ¤ì£¼ì‹œë©´..."

# Specific query â†’ Direct answer
response = await pipeline.process_message(
    "ê°œì¸ì •ë³´ë³´í˜¸ë²• ì œ15ì¡° ìˆ˜ì§‘ ë™ì˜ ì ˆì°¨ê°€ ê¶ê¸ˆí•©ë‹ˆë‹¤",
    conversation_id
)
# Response: Detailed legal advice with sources
```

### Multi-turn Conversation
```python
conv_manager = ConversationManager(pipeline)

# Start conversation
conv_id, response = await conv_manager.start_conversation(
    "ê¸ˆìœµìƒí’ˆ ë¶ˆì™„ì „íŒë§¤ ê´€ë ¨ ë¬¸ì˜"
)

# Continue conversation
response = await conv_manager.continue_conversation(
    conv_id,
    "êµ¬ì²´ì ìœ¼ë¡œ í€ë“œ íˆ¬ì ê´€ë ¨í•´ì„œ ì„¤ëª…ì˜ë¬´ ìœ„ë°˜ ì‹œ êµ¬ì œ ë°©ë²•ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤"
)

# Get conversation summary
summary = conv_manager.get_conversation_summary(conv_id)
```

### Status Monitoring
```python
status = await pipeline.get_pipeline_status()
conv_status = pipeline.get_conversation_status(conversation_id)
```

## Response Format

```python
class LegalResponse:
    answer: str                    # Main legal advice
    sources: List[SearchSource]    # Referenced sources
    confidence: float              # Response reliability (0-1)
    related_keywords: List[str]    # Related legal terms
    follow_up_questions: List[str] # Suggested next questions
    processing_time: float         # Processing duration
    query: LegalQuery             # Original processed query
```

## Development

Install dependencies:
```bash
cd llm
poetry install
```

Run tests:
```bash
poetry run pytest
```

## Integration with Main Project

Add to docker-compose.yml:
```yaml
llm-service:
  build:
    context: ./llm
  container_name: agentl2-llm
  environment:
    - LLM_OPENAI_API_KEY=${OPENAI_API_KEY}
  ports:
    - "8001:8000"
```

## Roadmap

- [ ] Integration with internal database search
- [ ] Advanced legal reasoning capabilities
- [ ] Multi-language support
- [ ] Legal document analysis
- [ ] Conversation memory persistence